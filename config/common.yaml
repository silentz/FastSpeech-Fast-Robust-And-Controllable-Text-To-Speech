seed_everything: 3407

trainer:
  gpus: 1
  check_val_every_n_epoch: 1
  fast_dev_run: false
  accumulate_grad_batches: 1
  gradient_clip_val: 1.
  max_epochs: 30
  precision: 32
  num_sanity_val_steps: 1
  deterministic: false
  auto_lr_find: false
  callbacks:
    - class_path: pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint
      init_args:
        dirpath: checkpoints/model/
        filename: "{epoch:03d}-{total_loss:.5f}"
        monitor: "total_loss"
        mode: "min"
        save_top_k: -1
    - class_path: pytorch_lightning.callbacks.LearningRateMonitor
      init_args:
        logging_interval: epoch

model:
  encoder_layers: 2
  decoder_layers: 2
  embedding_dim: 384
  attention_n_heads: 2
  attention_head_dim: 384
  dropout: 0.
  conv_hidden_size: 384
  n_mels: 80
  duration_hidden_size: 384
  len_reg_alpha: 1.
  optimizer_lr: 0.001

data:
  train_dataset:
    class_path: src.dataset.OverfitDataset
    init_args:
      dataset:
        class_path: src.dataset.AdvancedLJSpeechDataset
        init_args:
          root: 'data/'
          durations_path: ./checkpoints/durations.pt
      num_samples: 16
      length: 16000
  # train_dataset:
  #   class_path: src.dataset.AdvancedLJSpeechDataset
  #   init_args:
  #     root: 'data/'
  #     durations_path: ./checkpoints/durations.pt
  train_batch_size: 16
  train_num_workers: 16

  val_dataset:
    class_path: src.dataset.OverfitDataset
    init_args:
      dataset:
        class_path: src.dataset.AdvancedLJSpeechDataset
        init_args:
          root: 'data/'
          durations_path: ./checkpoints/durations.pt
      num_samples: 16
      length: 16
  val_batch_size: 16
  val_num_workers: 16

  # val_dataset:
  #   class_path: src.dataset.TestDataset
  # val_batch_size: 1
  # val_num_workers: 1
