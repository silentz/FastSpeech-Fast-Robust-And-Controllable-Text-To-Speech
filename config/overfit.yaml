seed_everything: 42

trainer:
  gpus: 1
  check_val_every_n_epoch: 1
  fast_dev_run: false
  accumulate_grad_batches: 2
  # gradient_clip_val: 1.
  track_grad_norm: 1
  max_epochs: 100
  precision: 32
  num_sanity_val_steps: 3
  deterministic: false
  auto_lr_find: false
  callbacks:
    - class_path: pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint
      init_args:
        dirpath: checkpoints/model/
        filename: "{epoch:03d}-{total_loss:.5f}"
        monitor: "total_loss"
        mode: "min"
        save_top_k: -1
    - class_path: pytorch_lightning.callbacks.LearningRateMonitor
      init_args:
        logging_interval: epoch

model:
  encoder_layers: 2
  decoder_layers: 2
  embedding_dim: 256
  attention_n_heads: 2
  attention_head_dim: 256
  dropout: 0.1
  conv_hidden_size: 1024
  n_mels: 80
  duration_hidden_size: 256
  len_reg_alpha: 1.
  optimizer_lr: 0.001

data:
  train_dataset:
    class_path: src.dataset.OverfitDataset
    init_args:
      dataset:
        class_path: src.dataset.AdvancedLJSpeechDataset
        init_args:
          root: 'data/'
          durations_path: ./checkpoints/durations.pt
      num_samples: 16
      length: 1600
  train_batch_size: 16
  train_num_workers: 16

  val_dataset:
    class_path: src.dataset.OverfitDataset
    init_args:
      dataset:
        class_path: src.dataset.TestLJSpeechDataset
        init_args:
          root: 'data/'
      num_samples: 3
      length: 3
  val_batch_size: 1
  val_num_workers: 1

  # val_dataset:
  #   class_path: src.dataset.OverfitDataset
  #   init_args:
  #     dataset:
  #       class_path: src.dataset.AdvancedLJSpeechDataset
  #       init_args:
  #         root: 'data/'
  #         durations_path: ./checkpoints/durations.pt
  #     num_samples: 16
  #     length: 16
  # val_batch_size: 16
  # val_num_workers: 16

  # val_dataset:
  #   class_path: src.dataset.TestDataset
  # val_batch_size: 1
  # val_num_workers: 1
